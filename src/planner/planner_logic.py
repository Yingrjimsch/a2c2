from prompts import prompts
from dto.instruction_dto import Instruction
from fastapi import UploadFile
from typing import List
from memory.memory import Memory
from memory.instruction_memory import InstructionMemory
import os
import openai
from debate.debate import Debate

# Import the LLMEndpoint class from the llm_api module
from llm_api import LLMEndpoint
from ui_components import UIComponents
import ast

"""
This module is responsible for the planning of the instructions.
"""
mem = Memory(os.path.join(os.path.dirname(__file__), 'memory/database.json'))
config_path = '../promptconfig.json'
llm = LLMEndpoint(openai.api_key, config_path=config_path)

def plan(instruction: Instruction, screens: List[UploadFile]):
    """
    This function plans the actions based on the instruction and the screens.
    Args:
        instruction (Instruction): The instruction to be planned.
        screens (List[UploadFile]): The list of screen images.
    Returns:
        Instruction: The planned instruction.
    """
    instructions, gui_representation = decompose(instruction, screens)
    instruction = select_instruction(instructions, "\n".join(gui_representation))
    if(instruction == "No more subinstructions to execute"):
        return None
    return instruction

# Function to decompose the instruction into subtasks and infer the dynamic actions based on the screens 
# and the memory of previous instructions 
def decompose(instruction: Instruction, screens: List[UploadFile]):
    """
    This function decomposes the instruction into subtasks and infers the dynamic actions based on the screens.
    Args:
        instruction (Instruction): The instruction to be decomposed.
        screens (List[UploadFile]): The list of screen images.
    Returns:
        List[Instruction]: The decomposed instruction and the generated instruction by LLM.
        List[str]: The inferred UI components.
        """
    ## First things first get current UI state
    gui_representation = dynamic_action_inference(screens)
    print("GUI Representation:", gui_representation)
    llm_instruction: list[str] = ast.literal_eval(get_llm_instruction(instruction, gui_representation))
    ## Then get the closest instruction (for now only once per instruction)
    if instruction.subinstructions_pointer == 0:
        instruction_memory = mem.get_top_results(instruction.instruction_text, k=1)
        if not instruction_memory == []:
            instruction = instruction_memory[0]
        else:
            instruction.subinstructions = llm_instruction
    # Then get the decomposed instruction generated by llm
    return [instruction, llm_instruction], gui_representation
    ## Thirdly break down the actions into subtasks

def get_llm_instruction(instruction: Instruction, gui_representation):
    """
    This function queries the LLM model with the instruction and the GUI representation and returns the decomposed instruction.
    Args:
        instruction (Instruction): The instruction to be decomposed.
        gui_representation (str): The GUI representation.
    Returns:
        str: The decomposed instruction.
    """
    system_prmpt = {"role": "system", "content": prompts["system_prompt_llm_decompose"]}
    user_prompt = {"role": "user", "content": prompts["user_prompt_llm_decompose"]
                   .replace("##instruction##", instruction.instruction_text)
                   .replace("##past_actions##", "\n".join(instruction.subinstructions[:instruction.subinstructions_pointer]))
                   .replace("##gui_representation##", "\n".join(gui_representation))}
    ## TODO: here openai call
    print("decompose prompt:", system_prmpt, user_prompt)
    response = openai.chat.completions.create(
                model="gpt-4o",
                messages=[system_prmpt, user_prompt],
                temperature=0,
                max_tokens=1200,
            )
    gen = response.choices[0].message.content.strip()
    return gen

def get_llm_subtasks(instruction: Instruction, gui_representation):
    """
    This function queries the LLM model with the instruction and the GUI representation and returns the subtasks.
    Args:
        instruction (Instruction): The instruction to be decomposed.
        gui_representation (str): The GUI representation.
    Returns:
        str: The subtasks.
    """
    system_prmpt = {"role": "system", "content": prompts["system_prompt_llm_plan"]}
    user_prompt = {"role": "user", "content": prompts["user_prompt_llm_plan"]
                   .replace("##instruction##", instruction.subinstructions[instruction.subinstructions_pointer])
                   .replace("##past_actions##", '\n'.join([subtask['action_description'] for subtask in instruction.subtasks[:instruction.subtask_pointer]]))
                   .replace("##gui_representation##", gui_representation)}
    ## TODO: here openai call
    print("subtask plan prompt:", system_prmpt, user_prompt)
    response = openai.chat.completions.create(
                model="gpt-4o",
                messages=[system_prmpt, user_prompt],
                temperature=0,
                max_tokens=1200,
            )
    gen = response.choices[0].message.content.strip()
    return gen

# Function to infer the dynamic actions based on the screens
def dynamic_action_inference(screens: List[UploadFile]):
    """
    This function queries the LLM model with the images of the screens and returns the inferred UI components.
    Args:
        screens (List[UploadFile]): List of screen images
    Returns:
        List[str]: List of inferred UI components
    """
    responses= []
    for screen in screens:
        #print(screen.file.read())
        response = llm.query(image_path=screen.file)
        # get only the content of the response
        print("Dynamic action inference with llm returns:", response)
        ui_componets = UIComponents(response)
        responses.append(str(ui_componets))
    return responses


# Function to select the plan based on the decomposed subtasks
def plan_select(subtasks: List[str]):
    return "Plan selection is not yet implemented!"

# Function to ground the plan by executing the actions on the screen
def ground(instruction: Instruction, gui_representation):
    """
    This function grounds the instruction to the action types.
    Args:
        instruction (Instruction): The instruction to be grounded.
        gui_representation (str): The GUI representation.
    Returns:
        Instruction: The grounded instruction.
    """
    # grounding with llm
    subtasks = get_llm_subtasks(instruction, gui_representation)
    print(subtasks)
    # append subtasks to the instruction
    instruction.subtasks.extend(ast.literal_eval(subtasks))
    return instruction

def select_instruction(instructions: list[Instruction], gui_representation: str):
    # Debate over the current instruction if the current or the newly generated one is better
    print("SELECT INSTRUCTION\n\n")
    print(instructions[0].subinstructions)
    print(instructions[1])
    if instructions[0].subinstructions_pointer < len(instructions[0].subinstructions):
        debate = Debate(instructions[0].instruction_text, 
                        gui_representation, 
                        [instructions[0].subinstructions[instructions[0].subinstructions_pointer], 
                        instructions[1][0]], api_key=openai.api_key, save_file_dir="")
        debate_answer, success = debate.run()
        debate.save_file_to_json("test")
        # Choose the right instruction after debate
        if success:
            instructions[0].subinstructions[instructions[0].subinstructions_pointer] = debate_answer
        # ground the instruction to action types
        instruction = ground(instructions[0], gui_representation)
        return instruction
    else:
        return "No more subinstructions to execute"
    ## TODO: Validate action!
    
    
